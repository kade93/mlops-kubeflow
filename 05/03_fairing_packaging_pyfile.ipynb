{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "619a3afc-e546-440c-bf81-1a50b1342f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting my_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_model.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime, os\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyModel(object):\n",
    "    def run(self):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(\"Using {} device\".format(device))\n",
    "\n",
    "        train_data = datasets.FashionMNIST(\n",
    "        root=\"/home/jovyan/mlops-kubeflow/data/FashionMNIST\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor(),\n",
    "        )\n",
    "\n",
    "        test_data = datasets.FashionMNIST(\n",
    "            root=\"/home/jovyan/mlops-kubeflow/data/FashionMNIST\",\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=transforms.ToTensor(),\n",
    "        )\n",
    "\n",
    "        BATCH_SIZE = 32\n",
    "        train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "        for (x_train, y_train) in train_dataloader:\n",
    "            print(\"Shape of X [N, C, H, W]: \", x_train.shape)\n",
    "            print(\"Shape of y: \", y_train.shape, y_train.dtype)\n",
    "            break\n",
    "\n",
    "        class NeuralNetwork(nn.Module):\n",
    "\n",
    "            def __init__(self):\n",
    "                super(NeuralNetwork, self).__init__()\n",
    "\n",
    "                self.flatten = nn.Flatten()\n",
    "                self.linear_relu_stack = nn.Sequential(\n",
    "                    nn.Linear(28*28, 512),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(512, 512),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(512, 10),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = self.flatten(x)\n",
    "                logits = self.linear_relu_stack(x)\n",
    "                output = F.log_softmax(logits, dim=1)\n",
    "                return output\n",
    "\n",
    "        model = NeuralNetwork().to(device)\n",
    "        print(model)\n",
    "\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "        def train(dataloader, model, loss_fn, optimizer):\n",
    "            size = len(dataloader.dataset)\n",
    "            for batch, (X, y) in enumerate(dataloader):\n",
    "                X, y = X.to(device), y.to(device)\n",
    "\n",
    "                # 예측 오류 계산\n",
    "                pred = model(X)\n",
    "                loss = loss_fn(pred, y)\n",
    "\n",
    "                # 역전파\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if batch % 500 == 0:\n",
    "                    loss, current = loss.item(), batch * len(X)\n",
    "                    # 텐서보드에 Train Loss / per epoch 로그 기록 \n",
    "                    writer.add_scalar('Train/Loss', loss, t+1)\n",
    "                    print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "        def test(dataloader, model, loss_fn):\n",
    "            size = len(dataloader.dataset)\n",
    "            num_batches = len(dataloader)\n",
    "            model.eval()\n",
    "            test_loss, correct = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for X, y in dataloader:\n",
    "                    X, y = X.to(device), y.to(device)\n",
    "                    pred = model(X)\n",
    "                    test_loss += loss_fn(pred, y).item()\n",
    "                    correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            test_loss /= num_batches\n",
    "            correct /= size\n",
    "            test_accuracy = 100. * correct \n",
    "            # 텐서보드에 Test 로그 기록\n",
    "            writer.add_scalar('Test/Loss', test_loss, t+1)\n",
    "            writer.add_scalar('Test/Accuracy', test_accuracy, t+1)\n",
    "            writer.flush()\n",
    "            print(f\"Test Result: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "        date_folder = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        # 분기설정 \n",
    "        if os.getenv('FAIRING_RUNTIME', None) is None:\n",
    "            log_dir = \"/home/jovyan/log/fit/\" + date_folder\n",
    "        else:\n",
    "            log_dir = \"/home/jovyan/job/log/fit/\" + date_folder  \n",
    "\n",
    "        print(f\"tensorboard log dir : {log_dir}\")\n",
    "\n",
    "        writer = SummaryWriter(log_dir)\n",
    "        epochs = 1\n",
    "\n",
    "        for t in range(epochs):\n",
    "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "            train(train_dataloader, model, loss_fn, optimizer)\n",
    "            test(test_dataloader, model, loss_fn)\n",
    "\n",
    "\n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c97d0cde-3209-426f-be59-5f545b0658a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from my_model import MyModel\n",
    "from kubeflow import fairing\n",
    "from kubeflow.fairing.kubernetes.utils import mounting_pvc\n",
    "\n",
    "DOCKER_REGISTRY = 'www.dolearn.io:30003/kade-kubeflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f20576c2-cf19-4b30-b036-971dc8d18b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_package():\n",
    "    my_model = MyModel()\n",
    "    my_model.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7159be17-c348-469d-bcc4-0f742b8df96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_map 에 key[현재경로의 파일이름]:value[컨테이너 안의 파일경로] 형태로 넣어줍니다.\n",
    "output_map =  {\n",
    "    \"my_model.py\": \"/app/my_model.py\"\n",
    "}            \n",
    "\n",
    "# preprocessor에서 ouput_map을 넣음으로써 fairing 패키지 안에 model_FashionMNIST.py가 들어가게 됩니다.\n",
    "fairing.config.set_preprocessor(\"function\", \n",
    "                                function_obj=train_with_package,\n",
    "                                output_map=output_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d45bd942-d55e-404d-9108-db707dbd999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairing.config.set_builder(\n",
    "    'append',\n",
    "    image_name='fashionmnist-packagedjob', \n",
    "    base_image='www.dolearn.io:30003/base/fairing-base:0.0.2',\n",
    "    registry=DOCKER_REGISTRY, \n",
    "    push=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b50777da-52ac-4b6f-bed7-f65a45f4c6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 211119 06:55:28 utils:51] The function mounting_pvc has been deprecated,                     please use `volume_mounts`\n"
     ]
    }
   ],
   "source": [
    "# fairing mounting pvc 추가\n",
    "notebook_volume = mounting_pvc(pvc_name=\"workspace-kade\", \n",
    "                                pvc_mount_path=\"/home/jovyan\") #마운트 경로 \n",
    "\n",
    "\n",
    "fairing.config.set_deployer('job',\n",
    "                            pod_spec_mutators=[notebook_volume],\n",
    "                            cleanup=False) # 잡을 실행후 완료시 잡을 삭제할지의 여부를 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "333dc944-b826-4f52-8edf-e799dfdd3060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 211119 06:55:28 config:134] Using preprocessor: <kubeflow.fairing.preprocessors.function.FunctionPreProcessor object at 0x7f3abda86dd8>\n",
      "[I 211119 06:55:28 config:136] Using builder: <kubeflow.fairing.builders.append.append.AppendBuilder object at 0x7f3ad1b2f0f0>\n",
      "[I 211119 06:55:28 config:138] Using deployer: <kubeflow.fairing.deployers.job.job.Job object at 0x7f3ac7cb7a20>\n",
      "[W 211119 06:55:28 append:52] Building image using Append builder...\n",
      "[I 211119 06:55:28 base:112] Creating docker context: /tmp/fairing_context_p8j82tm1\n",
      "[W 211119 06:55:28 base:99] /usr/local/lib/python3.6/dist-packages/kubeflow/fairing/__init__.py already exists in Fairing context, skipping...\n",
      "[I 211119 06:55:28 docker_creds_:234] Loading Docker credentials for repository 'www.dolearn.io:30003/base/fairing-base:0.0.2'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "Image name :  www.dolearn.io:30003/kade-kubeflow/fashionmnist-packagedjob:5BE42727\n",
      "==========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 211119 06:55:28 append:56] Image successfully built in 0.4822619769984158s.\n",
      "[W 211119 06:55:28 append:98] Pushing image www.dolearn.io:30003/kade-kubeflow/fashionmnist-packagedjob:5BE42727...\n",
      "[I 211119 06:55:28 docker_creds_:234] Loading Docker credentials for repository 'www.dolearn.io:30003/kade-kubeflow/fashionmnist-packagedjob:5BE42727'\n",
      "[W 211119 06:55:28 append:85] Uploading www.dolearn.io:30003/kade-kubeflow/fashionmnist-packagedjob:5BE42727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "Image name :  www.dolearn.io:30003/kade-kubeflow/fashionmnist-packagedjob:5BE42727\n",
      "==========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 211119 06:55:28 docker_session_:280] Layer sha256:3cf8fb62ba5ffb221a2edb2208741346eb4d2d99a174138e4afbb69ce1fd9966 exists, skipping\n",
      "[I 211119 06:55:28 docker_session_:280] Layer sha256:29d136a889d232058c476b5637c18cbfca74c586634cbee07fe71fa540c7b211 exists, skipping\n",
      "[I 211119 06:55:28 docker_session_:280] Layer sha256:0269b6883f78a00bb29875d37fe3d838dbbe61cadf0108145fff2be316364f74 exists, skipping\n",
      "[I 211119 06:55:28 docker_session_:280] Layer sha256:3caed8c8884bf3a0cd5255f42fec14c219153bcdf294c81cb2e0599298c8a8df exists, skipping\n",
      "[I 211119 06:55:28 docker_session_:280] Layer sha256:641afa4edc436e3fd3efd40433f1ad0c55b48af949680cd2359de51e3c439699 exists, skipping\n",
      "[I 211119 06:55:29 docker_session_:280] Layer sha256:02842a89d653002ea6c32f5573a9cec312ace226dae5eea21bc68782f4e2f627 exists, skipping\n",
      "[I 211119 06:55:29 docker_session_:280] Layer sha256:f5098a9bf4490bccac9085b1bf9c54baf3015333c40fb6685889a9785b7388ee exists, skipping\n",
      "[I 211119 06:55:29 docker_session_:280] Layer sha256:40c455d0dacc33a87519926f4749deef90e15bca99118d8bf7d8cf78588f7f9b exists, skipping\n",
      "[I 211119 06:55:29 docker_session_:280] Layer sha256:e80c964ece6a3edf0db1cfc72ae0e6f0699fb776bbfcc92b708fbb945b0b9547 exists, skipping\n",
      "[I 211119 06:55:29 docker_session_:280] Layer sha256:191b6069f9932358899ffcc3f45c41a0f5f2731b948236a6553484caaf989794 exists, skipping\n",
      "[I 211119 06:55:29 docker_session_:280] Layer sha256:7ada0795a7988a0d48120cfe85bc57dba3bdd225474db83b4e5565b4af8dd0a9 exists, skipping\n",
      "[I 211119 06:55:29 docker_session_:280] Layer sha256:8bcf82863cb9582a24dc32cd3ddf560ff2f84df88694be072758159b94b70bd3 exists, skipping\n",
      "[I 211119 06:55:29 docker_session_:280] Layer sha256:a4dd3c805ec24b016ac8a3869add24541829736c312b65bd49d3b2af7501f897 exists, skipping\n",
      "[I 211119 06:55:29 docker_session_:280] Layer sha256:9151e6e2942f84adfd723f3117577c80e9bd90fab642b2190a5e501fe01f534a exists, skipping\n",
      "[I 211119 06:55:29 docker_session_:280] Layer sha256:e1b8f4d5dcdfb4ac873d37d3a643cba6a55f2b325cfe0115aaba32946e896e0a exists, skipping\n",
      "[I 211119 06:55:29 docker_session_:280] Layer sha256:063a4ff324e290814ea5bf23d5f8de5cca1a734782c4a187132ab3364b44a985 exists, skipping\n",
      "[I 211119 06:55:29 docker_session_:280] Layer sha256:f22ccc0b8772d8e1bcb40f137b373686bc27427a70c0e41dd22b38016e09e7e0 exists, skipping\n",
      "[I 211119 06:55:29 docker_session_:280] Layer sha256:4bf23ae646f0b9d8e07bf427c69c82f208bb57a8b297507d9b8b6fa23b725711 exists, skipping\n",
      "[I 211119 06:55:29 docker_session_:280] Layer sha256:7a12503ba844465b2c5aea7ebf60dd5057c7fcece51ea15e5f7f02ed1ae08d12 exists, skipping\n",
      "[I 211119 06:55:29 docker_session_:280] Layer sha256:208b1b1d503e89fb2452c622d99f8a69b643819c098688dd89a4bce51d843f7d exists, skipping\n",
      "[I 211119 06:55:29 docker_session_:280] Layer sha256:f571d568b0961b0954a50f361ad842acab3b6e4b21a27430e172a1f0d5aca5db exists, skipping\n",
      "[I 211119 06:55:29 docker_session_:284] Layer sha256:aaa28a8b5a751b0b3ef1a9be036a5eff18c9f928a20d02f865ad1d7b5dacbe2f pushed.\n",
      "[I 211119 06:55:29 docker_session_:284] Layer sha256:dc8e028e2ad7949f11d453e7530619a9fe2800eadc37ae810dd0ae369af92f59 pushed.\n",
      "[I 211119 06:55:30 docker_session_:334] Finished upload of: www.dolearn.io:30003/kade-kubeflow/fashionmnist-packagedjob:5BE42727\n",
      "[W 211119 06:55:30 append:103] Pushed image www.dolearn.io:30003/kade-kubeflow/fashionmnist-packagedjob:5BE42727 in 1.522401018999517s.\n",
      "[W 211119 06:55:30 job:101] The job fairing-job-bn2qq launched.\n",
      "[W 211119 06:55:30 manager:298] Waiting for fairing-job-bn2qq-bljnb to start...\n",
      "[W 211119 06:55:30 manager:298] Waiting for fairing-job-bn2qq-bljnb to start...\n",
      "[W 211119 06:55:30 manager:298] Waiting for fairing-job-bn2qq-bljnb to start...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "Building image www.dolearn.io:30003/kade-kubeflow/fashionmnist-packagedjob:5BE42727 done.\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 211119 06:55:31 manager:298] Waiting for fairing-job-bn2qq-bljnb to start...\n",
      "[I 211119 06:55:32 manager:304] Pod started running True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Shape of X [N, C, H, W]:  torch.Size([32, 1, 28, 28])\n",
      "Shape of y:  torch.Size([32]) torch.int64\n",
      "NeuralNetwork(\n",
      "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n",
      "tensorboard log dir : /home/jovyan/job/log/fit/20211119-065533\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.308242  [    0/60000]\n",
      "loss: 2.266033  [16000/60000]\n",
      "loss: 2.172069  [32000/60000]\n",
      "loss: 2.249936  [48000/60000]\n",
      "Test Result:\n",
      " Accuracy: 51.2%, Avg loss: 2.074145\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    fairing.config.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac7dd13-a3c3-48d5-ba8a-ed49b21ab718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
