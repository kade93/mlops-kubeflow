{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dfa5db5-faba-4ccf-a912-c1e16b502f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing my_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_model.py\n",
    "\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import datetime, os\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyModel(object):\n",
    "    def run(self):    \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#         print(\"Using {} device\".format(device))\n",
    "\n",
    "        train_data = datasets.FashionMNIST(\n",
    "            root=\"/home/jovyan/mlops-kubeflow/data/FashionMNIST\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.ToTensor(),\n",
    "        )\n",
    "\n",
    "        test_data = datasets.FashionMNIST(\n",
    "            root=\"/home/jovyan/mlops-kubeflow/data/FashionMNIST\",\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=transforms.ToTensor(),\n",
    "        )\n",
    "\n",
    "        BATCH_SIZE = 32\n",
    "        train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "#         for (x_train, y_train) in train_dataloader:\n",
    "#             print(\"Shape of X [N, C, H, W]: \", x_train.shape)\n",
    "#             print(\"Shape of y: \", y_train.shape, y_train.dtype)\n",
    "#             break\n",
    "\n",
    "        #     plt.figure(figsize=(10, 1))\n",
    "        #     for i in range(10):\n",
    "        #         plt.subplot(1, 10, i + 1)\n",
    "        #         plt.imshow(x_train[i, :, :, :].numpy().reshape(28, 28), cmap = \"gray_r\")\n",
    "        #         plt.title(\"class: \" + str(y_train[i].item()))\n",
    "        #         plt.axis(\"off\")\n",
    "\n",
    "        class NeuralNetwork(nn.Module):\n",
    "\n",
    "            def __init__(self):\n",
    "                super(NeuralNetwork, self).__init__()\n",
    "\n",
    "                self.flatten = nn.Flatten()\n",
    "                self.linear_relu_stack = nn.Sequential(\n",
    "                    nn.Linear(28*28, 512),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(512, 512),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(512, 10),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = self.flatten(x)\n",
    "                logits = self.linear_relu_stack(x)\n",
    "                output = F.log_softmax(logits, dim=1)\n",
    "                return output\n",
    "\n",
    "        model = NeuralNetwork().to(device)\n",
    "#         print(model)\n",
    "\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "        def train(dataloader, model, loss_fn, optimizer):\n",
    "            size = len(dataloader.dataset)\n",
    "            for batch, (X, y) in enumerate(dataloader):\n",
    "                X, y = X.to(device), y.to(device)\n",
    "\n",
    "                # 예측 오류 계산\n",
    "                pred = model(X)\n",
    "                loss = loss_fn(pred, y)\n",
    "\n",
    "                # 역전파\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if batch % 500 == 0:\n",
    "                    loss, current = loss.item(), batch * len(X)\n",
    "                    # 텐서보드에 Train Loss / per epoch 로그 기록 \n",
    "                    writer.add_scalar('Train/Loss', loss, t+1)\n",
    "                    print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "        def test(dataloader, model, loss_fn):\n",
    "            size = len(dataloader.dataset)\n",
    "            num_batches = len(dataloader)\n",
    "            model.eval()\n",
    "            test_loss, correct = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for X, y in dataloader:\n",
    "                    X, y = X.to(device), y.to(device)\n",
    "                    pred = model(X)\n",
    "                    test_loss += loss_fn(pred, y).item()\n",
    "                    correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            test_loss /= num_batches\n",
    "            correct /= size\n",
    "            test_accuracy = 100. * correct \n",
    "            # 텐서보드에 Test 로그 기록\n",
    "            writer.add_scalar('Test/Loss', test_loss, t+1)\n",
    "            writer.add_scalar('Test/Accuracy', test_accuracy, t+1)\n",
    "            writer.flush()\n",
    "            print(f\"Test Result: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "        date_folder = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        # 분기설정 \n",
    "        if os.getenv('FAIRING_RUNTIME', None) is None:\n",
    "            log_dir = \"/home/jovyan/log/fit/\" + date_folder\n",
    "        else:\n",
    "            log_dir = \"/home/jovyan/job/log/fit/\" + date_folder  \n",
    "\n",
    "        print(f\"tensorboard log dir : {log_dir}\")\n",
    "\n",
    "        writer = SummaryWriter(log_dir)\n",
    "        epochs = 1\n",
    "\n",
    "        for t in range(epochs):\n",
    "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "            train(train_dataloader, model, loss_fn, optimizer)\n",
    "            test(test_dataloader, model, loss_fn)\n",
    "\n",
    "\n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c80d3b6f-54dc-41c1-aa52-25faed604fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from my_model import MyModel\n",
    "from kubeflow import fairing\n",
    "from kubeflow.fairing.kubernetes.utils import mounting_pvc\n",
    "\n",
    "DOCKER_REGISTRY = 'www.dolearn.io:30003/kade-kubeflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c43eae39-1094-46a5-99c4-2bf24160a100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_package():\n",
    "    my_model = MyModel()\n",
    "    my_model.run()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74a88a30-094c-4f09-b8ad-ca335f2a1d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_map 에 key[현재경로의 파일이름]:value[컨테이너 안의 파일경로] 형태로 넣어줍니다.\n",
    "output_map =  {\n",
    "    \"my_model.py\": \"/app/my_model.py\"\n",
    "}            \n",
    "\n",
    "# preprocessor에서 ouput_map을 넣음으로써 fairing 패키지 안에 model_FashionMNIST.py가 들어가게 됩니다.\n",
    "fairing.config.set_preprocessor(\"function\", \n",
    "                                function_obj=train_with_package,\n",
    "                                output_map=output_map)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80aefb05-eff8-4b27-b3fa-32a887cdb3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairing.config.set_builder(\n",
    "    'append',\n",
    "    image_name='fashionmnist-packagedjob', \n",
    "    base_image='www.dolearn.io:30003/base/fairing-base:0.0.2',\n",
    "    registry=DOCKER_REGISTRY, \n",
    "    push=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d49ea02-f88e-45af-9c55-52646b2de455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 211119 04:31:00 utils:51] The function mounting_pvc has been deprecated,                     please use `volume_mounts`\n"
     ]
    }
   ],
   "source": [
    "# fairing mounting pvc 추가\n",
    "notebook_volume = mounting_pvc(pvc_name=\"workspace-kade\", \n",
    "                                pvc_mount_path=\"/home/jovyan\") #마운트 경로 /notebook \n",
    "\n",
    "\n",
    "fairing.config.set_deployer('job',\n",
    "                            pod_spec_mutators=[notebook_volume],\n",
    "                            cleanup=False) # 잡을 실행후 완료시 잡을 삭제할지의 여부를 결정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "818d596d-7597-42b1-a5d8-ce32a9c1cf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 211119 04:31:00 config:134] Using preprocessor: <kubeflow.fairing.preprocessors.function.FunctionPreProcessor object at 0x7fa3ec375208>\n",
      "[I 211119 04:31:00 config:136] Using builder: <kubeflow.fairing.builders.append.append.AppendBuilder object at 0x7fa3fdac45c0>\n",
      "[I 211119 04:31:00 config:138] Using deployer: <kubeflow.fairing.deployers.job.job.Job object at 0x7fa3f8bd55c0>\n",
      "[W 211119 04:31:00 append:52] Building image using Append builder...\n",
      "[I 211119 04:31:00 base:112] Creating docker context: /tmp/fairing_context_1fi7hi2z\n",
      "[W 211119 04:31:00 base:99] /usr/local/lib/python3.6/dist-packages/kubeflow/fairing/__init__.py already exists in Fairing context, skipping...\n",
      "[I 211119 04:31:00 docker_creds_:234] Loading Docker credentials for repository 'www.dolearn.io:30003/base/fairing-base:0.0.2'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "Image name :  www.dolearn.io:30003/kade-kubeflow/fashionmnist-packagedjob:7019CB19\n",
      "==========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 211119 04:31:01 append:56] Image successfully built in 0.5640066569994815s.\n",
      "[W 211119 04:31:01 append:98] Pushing image www.dolearn.io:30003/kade-kubeflow/fashionmnist-packagedjob:7019CB19...\n",
      "[I 211119 04:31:01 docker_creds_:234] Loading Docker credentials for repository 'www.dolearn.io:30003/kade-kubeflow/fashionmnist-packagedjob:7019CB19'\n",
      "[W 211119 04:31:01 append:85] Uploading www.dolearn.io:30003/kade-kubeflow/fashionmnist-packagedjob:7019CB19\n",
      "[I 211119 04:31:01 docker_session_:280] Layer sha256:7ada0795a7988a0d48120cfe85bc57dba3bdd225474db83b4e5565b4af8dd0a9 exists, skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "Image name :  www.dolearn.io:30003/kade-kubeflow/fashionmnist-packagedjob:7019CB19\n",
      "==========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 211119 04:31:01 docker_session_:280] Layer sha256:e1b8f4d5dcdfb4ac873d37d3a643cba6a55f2b325cfe0115aaba32946e896e0a exists, skipping\n",
      "[I 211119 04:31:01 docker_session_:280] Layer sha256:f571d568b0961b0954a50f361ad842acab3b6e4b21a27430e172a1f0d5aca5db exists, skipping\n",
      "[I 211119 04:31:01 docker_session_:280] Layer sha256:063a4ff324e290814ea5bf23d5f8de5cca1a734782c4a187132ab3364b44a985 exists, skipping\n",
      "[I 211119 04:31:01 docker_session_:280] Layer sha256:7a12503ba844465b2c5aea7ebf60dd5057c7fcece51ea15e5f7f02ed1ae08d12 exists, skipping\n",
      "[I 211119 04:31:01 docker_session_:280] Layer sha256:3cf8fb62ba5ffb221a2edb2208741346eb4d2d99a174138e4afbb69ce1fd9966 exists, skipping\n",
      "[I 211119 04:31:01 docker_session_:280] Layer sha256:3caed8c8884bf3a0cd5255f42fec14c219153bcdf294c81cb2e0599298c8a8df exists, skipping\n",
      "[I 211119 04:31:01 docker_session_:280] Layer sha256:29d136a889d232058c476b5637c18cbfca74c586634cbee07fe71fa540c7b211 exists, skipping\n",
      "[I 211119 04:31:01 docker_session_:280] Layer sha256:208b1b1d503e89fb2452c622d99f8a69b643819c098688dd89a4bce51d843f7d exists, skipping\n",
      "[I 211119 04:31:01 docker_session_:280] Layer sha256:191b6069f9932358899ffcc3f45c41a0f5f2731b948236a6553484caaf989794 exists, skipping\n",
      "[I 211119 04:31:01 docker_session_:280] Layer sha256:40c455d0dacc33a87519926f4749deef90e15bca99118d8bf7d8cf78588f7f9b exists, skipping\n",
      "[I 211119 04:31:01 docker_session_:280] Layer sha256:8bcf82863cb9582a24dc32cd3ddf560ff2f84df88694be072758159b94b70bd3 exists, skipping\n",
      "[I 211119 04:31:01 docker_session_:280] Layer sha256:f22ccc0b8772d8e1bcb40f137b373686bc27427a70c0e41dd22b38016e09e7e0 exists, skipping\n",
      "[I 211119 04:31:01 docker_session_:280] Layer sha256:0269b6883f78a00bb29875d37fe3d838dbbe61cadf0108145fff2be316364f74 exists, skipping\n",
      "[I 211119 04:31:01 docker_session_:280] Layer sha256:641afa4edc436e3fd3efd40433f1ad0c55b48af949680cd2359de51e3c439699 exists, skipping\n",
      "[I 211119 04:31:01 docker_session_:280] Layer sha256:a4dd3c805ec24b016ac8a3869add24541829736c312b65bd49d3b2af7501f897 exists, skipping\n",
      "[I 211119 04:31:01 docker_session_:280] Layer sha256:4bf23ae646f0b9d8e07bf427c69c82f208bb57a8b297507d9b8b6fa23b725711 exists, skipping\n",
      "[I 211119 04:31:01 docker_session_:280] Layer sha256:f5098a9bf4490bccac9085b1bf9c54baf3015333c40fb6685889a9785b7388ee exists, skipping\n",
      "[I 211119 04:31:01 docker_session_:280] Layer sha256:e80c964ece6a3edf0db1cfc72ae0e6f0699fb776bbfcc92b708fbb945b0b9547 exists, skipping\n",
      "[I 211119 04:31:01 docker_session_:280] Layer sha256:02842a89d653002ea6c32f5573a9cec312ace226dae5eea21bc68782f4e2f627 exists, skipping\n",
      "[I 211119 04:31:02 docker_session_:280] Layer sha256:9151e6e2942f84adfd723f3117577c80e9bd90fab642b2190a5e501fe01f534a exists, skipping\n",
      "[I 211119 04:31:02 docker_session_:284] Layer sha256:4f0ce4c61e10cb3c508c45ebcbfeb2ec90df15a642309bcbf36aaa99ca215c55 pushed.\n",
      "[I 211119 04:31:02 docker_session_:284] Layer sha256:4f23eaf0fd36288336ef715fc64f122fcd71999385d235f2d44ac81d4df78218 pushed.\n",
      "[I 211119 04:31:02 docker_session_:334] Finished upload of: www.dolearn.io:30003/kade-kubeflow/fashionmnist-packagedjob:7019CB19\n",
      "[W 211119 04:31:02 append:103] Pushed image www.dolearn.io:30003/kade-kubeflow/fashionmnist-packagedjob:7019CB19 in 1.5898627040005522s.\n",
      "[W 211119 04:31:02 job:101] The job fairing-job-7qwsz launched.\n",
      "[W 211119 04:31:02 manager:298] Waiting for fairing-job-7qwsz-wgntv to start...\n",
      "[W 211119 04:31:02 manager:298] Waiting for fairing-job-7qwsz-wgntv to start...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "Building image www.dolearn.io:30003/kade-kubeflow/fashionmnist-packagedjob:7019CB19 done.\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 211119 04:31:02 manager:298] Waiting for fairing-job-7qwsz-wgntv to start...\n",
      "[W 211119 04:31:03 manager:298] Waiting for fairing-job-7qwsz-wgntv to start...\n",
      "[I 211119 04:31:05 manager:304] Pod started running True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard log dir : /notebook/log/fit/20211119-043106\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.305484  [    0/60000]\n",
      "loss: 2.292389  [16000/60000]\n",
      "loss: 2.291755  [32000/60000]\n",
      "loss: 2.251377  [48000/60000]\n",
      "Test Result:\n",
      " Accuracy: 23.6%, Avg loss: 2.248289\n",
      "\n",
      "Done!\n",
      "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    fairing.config.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfa9529-1d2f-4dd1-a658-1cef25b28469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
